{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hylite\n",
    "from hylite import io\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### From image to cloud, and visa versa\n",
    "\n",
    "One of the main purposes of hylite is to facilitate the projection of point cloud attributes onto hyperspectral images, and the back-projection of hyperspectral image data onto point clouds to derive georeferenced 3-D hyperclouds.\n",
    "\n",
    "The following notebook outlines the various ways that data can be moved between these data structures, and finishes by showing how the position of an image can be located relative to a georeferenced point cloud using computer vision techniques."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Cloud to image\n",
    "\n",
    "Moving data from point clouds onto an image is a standard computer graphics operation, known as rendering. This can be used to create images of e.g. surface orientation or position (both very useful properties for e.g. topographic corrections)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load a point cloud dataset\n",
    "cloud = io.load( 'test_data/hypercloud.ply' )\n",
    "cloud.decompress() # this was compressed from float to integer to save space; so we need to convert it back\n",
    "\n",
    "cam = cloud.header.get_camera(0) # get the camera pose used for rendering (more on this later)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create images with different cloud attributes in them\n",
    "attr = [\n",
    "    'rgb', # cloud RGB\n",
    "    'klm', # cloud normals\n",
    "    'xyz', # point positions\n",
    "    (0,25,40), # cloud attributes [ in this case, these will be hyperspectral bands as this is a hypercloud ]\n",
    "]\n",
    "images = []\n",
    "for a in attr:\n",
    "    images.append( cloud.render( cam, a, s=1,  fill_holes=True)) # N.B. adjust 's' to change the point size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hylite import HyImage\n",
    "HyImage.quick_plot?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images[0].quick_plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images[0].quick_plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images[0].quick_plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot\n",
    "fig,ax = plt.subplots(2,2,figsize=(18,10))\n",
    "for i,a,t in zip(images,ax.ravel(),attr):\n",
    "    i.set_as_nan(0) # replace background with nan\n",
    "    i.quick_plot((0,1,2), ax=a, \n",
    "                 vmin=2, vmax=98, # as these are integers, they will be treated as percentile clip values.\n",
    "                 tscale=True) #N.B. tscale=True means each band is scaled separately for visualising.\n",
    "    a.set_title(t)\n",
    "fig.tight_layout()\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These rendered attributes can then be used to calculate other important properties, e.g. the target - sensor distance for each pixel in a hyperspectral scene:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "depths = cloud.render(cam, bands='xyz',fill_holes=True ) # render point position\n",
    "depths.set_as_nan(0) # remove zeros\n",
    "depths.data -= cam.pos # put camera at origin\n",
    "depths.data = np.dstack( [ depths.data , np.linalg.norm(depths.data, axis=-1 ) ] ) # compute distance from sensor\n",
    "depths.set_band_names(['x','y','z','depth']) # add band names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot depth\n",
    "fig,ax = depths.quick_plot('depth')\n",
    "fig.colorbar(ax.cbar) # N.B. colorbar information is added to the relevant axes object as the .cbar attribute\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Image to cloud\n",
    "\n",
    "Similarly, the *project(...)* function can be used to back-project image data onto a point cloud."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = images[0].copy() # pretend this is some fancy hyperspectral sensor data\n",
    "image.data = 1-image.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cloud.data = None # clear previous data [ otherwise bands would be appended ]\n",
    "cloud.project( image, cam, bands=(0,1,2),trim=False ) # project bands 0, 1 and 2 onto the point cloud\n",
    "print(cloud.data.shape) # data has now been projected onto the point clouds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = cloud.quick_plot((0,1,2), 'ortho', s=3 ) \n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Locating sensor position\n",
    "\n",
    "The above methods only work if the sensor position and orientation are known. While this could theoretically be measured, it is challenging to do accurately. Instead, *hylite* provides two keypoint based methods for solving camera position.\n",
    "\n",
    "The first of these uses manually selected keypoint pairs to associate real-world coordinates with >4 image pixels and thus solve the sensor position and orientation. Note that: (1) image dimensions and sensor FOV must be known in advance, though these are generally reported by sensor manufacturers, and (2) lens distortions should be corrected prior to applying this alignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load an example image. This is just a .jpg (to keep file size low), but could equally be a hyperspectral image\n",
    "image2 = io.load('./test_data/scene.jpg') \n",
    "fig,ax = plt.subplots(figsize=(5,5))\n",
    "image2.quick_plot((0,1,2),ax=ax)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Manual matching\n",
    "\n",
    "CloudCompare can be used to interactively select points in the point cloud and get the indices of points representing features identifyable in the image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keypoints as cloud IDs\n",
    "points = np.array([137168, 64179, 194030, 38452, 18604,\n",
    "                  169834, 208316, 217343, 217344, 250920, 318733, ] )\n",
    "\n",
    "# keypoints as image pixel coordinates\n",
    "pixels = [(196,225),(202,251),(63,222),(215,319),(257,301),\n",
    "          (138,178),(46,185),(220,166),(348,170),(317,195),(359,211)]\n",
    "\n",
    "# define new camera object to store results in (and define sensor properties)\n",
    "from hylite.project import Camera\n",
    "cam2 = Camera(np.zeros(3), np.zeros(3),\n",
    "             'pano', # this is a tripod-mounted (panoroamic) sensor. Set to 'persp' for frame sensors.\n",
    "             fov = 32.3, # vertical sensor field of view\n",
    "             dims = (image2.xdim(), image2.ydim()),\n",
    "             step = 0.084 # angular step [ provided by manufacture, or assume square pixels ]\n",
    "             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hylite.project.align import align_to_cloud_manual\n",
    "est, r = align_to_cloud_manual( cloud, cam2, points, pixels) # solve camera pose using PnP solution\n",
    "print('Aligned camera with %.1f pixel residual.' % r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(1,2,figsize=(12,10))\n",
    "image2.quick_plot((0,1,2),ax=ax[0])\n",
    "cloud.quick_plot('rgb', est, ax=ax[1])\n",
    "ax[0].set_title(\"Image\")\n",
    "ax[1].set_title(\"View from estimated camera\")\n",
    "for px,py in pixels:\n",
    "    ax[0].scatter(px,py)\n",
    "    ax[1].scatter(px,py)\n",
    "for a in ax:\n",
    "    a.set_ylim(350,100) # zoom in a bit\n",
    "fig.tight_layout()\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Automated matching\n",
    "\n",
    "If an estimated camera position is available, e.g. using field measurements or by visualising the point-cloud in CloudCompare and noting the approximate camera position and orientation (hylite uses the same Euler angle scheme as CloudCompare for representing orientations) then automatic matching techniques (SIFT or ORB) can be used to match keypoints between the (rendered) point cloud and the image. \n",
    "\n",
    "This can be a fiddly process, but when it works it can greatly improve the accuracy with which sensor pose can be estimated. In the following we use this technique to refine our initial estimate of the camera pose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hylite.project.align import align_to_cloud\n",
    "est2, kp, r = align_to_cloud( image2, cloud, est, bands=(0,1,2), \n",
    "                             method='sift', # which keypoint extractor to use\n",
    "                             sf=2, # supersample rendered point cloud to improve matching (sometimes)\n",
    "                             s=2, # size of points when rendering cloud\n",
    "                             recurse=1, # repeatedly render cloud to improve/update matching based on new pose\n",
    "                             gf=True) # display graphical QAQC plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(1,2,figsize=(12,10))\n",
    "image2.quick_plot((0,1,2),ax=ax[0])\n",
    "cloud.quick_plot('rgb', est2, ax=ax[1])\n",
    "ax[0].set_title(\"Image\")\n",
    "ax[1].set_title(\"View from aligned camera\")\n",
    "for a in ax:\n",
    "    a.set_ylim(350,100) # zoom in a bit\n",
    "fig.tight_layout()\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HyScene objects\n",
    "\n",
    "To facilitate fusing clouds and images and manage associated datasets (e.g. per-pixel depths) hylite has a special type of *HyCollection* specifically for a coregistered image-cloud pair. These can facilitate e.g. illumination correction or hypercloud projection workflows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hylite import HyScene"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "S = HyScene('myscene', './outputs/') # initialise a scene just like any HyCollection\n",
    "S.construct( image2, cloud, est2 ) # do projections and construct scene"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "S.print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot per-pixel depths\n",
    "plt.imshow( S.depth.T, cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# project image data to cloud\n",
    "cloud2 = S.push_to_cloud( (0,1,2), method='average' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = cloud2.quick_plot( (0,1,2), 'ortho', s=4 )\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "S.save() # save HyScene for later use"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
